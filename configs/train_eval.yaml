
# ==== Model and Dataset ===
base_model: llava-hf/llava-v1.6-mistral-7b-hf # Based VLM for ORPO finetuning/alignment. unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit, llava-hf/llava-v1.6-mistral-7b-hf
dataset_name: openbmb/RLAIF-V-Dataset                 # ORPO style dataset. It mush have atleast columns ['question', 'image', 'accepted', 'rejected']

# === Training Options ===
evaluation: true        # Flat for model evaluation after training
max_seq_length: 4096    # Choose any! Unsloth auto support RoPE Scaling internally!
load_in_4bit: true      # Use 4bit quantization to reduce memory usage. Can be False.
load_in_8bit: false     # Use 4bit quantization to reduce memory usage. Can be False.
full_finetuning: false  # For full parameters finetuning. LoRA/QLoRA will not be considered 
dtype: null             # null = auto; use "float16" or "bfloat16" for override
num_train_epochs: 1     # full training runs (complete data is seen). Recommended [1-3], otherwise overfitting can occour
max_steps: 10

# === Output & Logging ===
output_dir: outputs     # directory for saving the checkpoints, model, ...
run_name: llava-v1.6-mistral-7b-4bit-r16
wandb_project: FineTuning-llava-v1.6-mistral-7b

# Optional if not using .env
wandb_api_key: YOUR_WANDB_TOKEN   # key for registering logs in wandb. You can also put 'wandb_api_key' in .env file

# === LoRA Parameters ===
finetune_vision_layers: true      # False if not finetuning vision layers
finetune_language_layers: true    # False if not finetuning language layers
finetune_attention_modules: true  # False if not finetuning attention layers
finetune_mlp_modules: true        # False if not finetuning MLP layers
r: 16                             # The larger, the higher the accuracy, but might overfit. Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
lora_alpha: 16                    # LoRA Rank * 1 or 2. scales the weights of the adapters (more influence on base model)
lora_dropout: 0.0                 # Supports any, but = 0 is optimized. Dropout rate to prevent overfitting
